"""
Intelligent test result analysis and advanced reporting for SQL unit testing framework.

This module provides AI-powered analysis of test results, pattern detection,
recommendation generation, and intelligent insights for test optimization.
"""
import logging
import statistics
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple, Set
from dataclasses import dataclass, field
from enum import Enum
import json
from collections import defaultdict, Counter
import re

from .models import TestResult, TestStatus, TestSuite
from .orchestration import WorkflowExecution

logger = logging.getLogger(__name__)


class AnalysisType(str, Enum):
    """Types of analysis performed on test results."""
    FAILURE_PATTERNS = "failure_patterns"
    PERFORMANCE_TRENDS = "performance_trends"
    COVERAGE_ANALYSIS = "coverage_analysis"
    FLAKY_TESTS = "flaky_tests"
    CORRELATION_ANALYSIS = "correlation_analysis"
    REGRESSION_DETECTION = "regression_detection"
    OPTIMIZATION_OPPORTUNITIES = "optimization_opportunities"


class SeverityLevel(str, Enum):
    """Severity levels for analysis findings."""
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    INFO = "info"


class RecommendationType(str, Enum):
    """Types of recommendations generated by analysis."""
    FIX_FLAKY_TEST = "fix_flaky_test"
    OPTIMIZE_PERFORMANCE = "optimize_performance"
    IMPROVE_COVERAGE = "improve_coverage"
    REFACTOR_TEST = "refactor_test"
    ADD_ASSERTION = "add_assertion"
    REVIEW_DEPENDENCIES = "review_dependencies"
    UPDATE_FIXTURES = "update_fixtures"


@dataclass
class AnalysisInsight:
    """Represents an insight discovered during analysis."""
    insight_id: str
    analysis_type: AnalysisType
    severity: SeverityLevel
    title: str
    description: str
    affected_tests: List[str] = field(default_factory=list)
    metrics: Dict[str, Any] = field(default_factory=dict)
    confidence_score: float = 1.0
    timestamp: datetime = field(default_factory=datetime.now)


@dataclass
class TestRecommendation:
    """Represents a recommendation for test improvement."""
    recommendation_id: str
    recommendation_type: RecommendationType
    title: str
    description: str
    target_tests: List[str] = field(default_factory=list)
    priority: SeverityLevel = SeverityLevel.MEDIUM
    effort_estimate: str = "medium"  # low, medium, high
    expected_impact: str = "medium"  # low, medium, high
    implementation_steps: List[str] = field(default_factory=list)
    confidence_score: float = 1.0


@dataclass
class TestAnalysisReport:
    """Comprehensive analysis report for test execution."""
    report_id: str
    execution_context: str
    analysis_timestamp: datetime
    summary: Dict[str, Any]
    insights: List[AnalysisInsight] = field(default_factory=list)
    recommendations: List[TestRecommendation] = field(default_factory=list)
    trends: Dict[str, Any] = field(default_factory=dict)
    quality_score: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)


class IntelligentTestAnalyzer:
    """AI-powered analyzer for intelligent test result analysis."""

    def __init__(self):
        """Initialize the intelligent test analyzer."""
        self.historical_data: Dict[str, List[TestResult]] = defaultdict(list)
        self.analysis_cache: Dict[str, TestAnalysisReport] = {}
        self.pattern_library: Dict[str, Dict[str, Any]] = self._initialize_pattern_library()

    def analyze_test_results(self,
                           results: List[TestResult],
                           context: str = "default",
                           historical_depth: int = 30) -> TestAnalysisReport:
        """
        Perform comprehensive analysis of test results.

        Args:
            results: List of test results to analyze
            context: Context identifier for the analysis
            historical_depth: Number of days to include in historical analysis

        Returns:
            Comprehensive analysis report with insights and recommendations
        """
        report_id = f"analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        # Store results for future analysis
        for result in results:
            self.historical_data[result.test_name].append(result)

        # Perform various analyses
        insights = []
        recommendations = []

        # Failure pattern analysis
        failure_insights = self._analyze_failure_patterns(results)
        insights.extend(failure_insights)

        # Performance trend analysis
        performance_insights = self._analyze_performance_trends(results, historical_depth)
        insights.extend(performance_insights)

        # Flaky test detection
        flaky_insights = self._detect_flaky_tests(results, historical_depth)
        insights.extend(flaky_insights)

        # Coverage analysis
        coverage_insights = self._analyze_test_coverage(results)
        insights.extend(coverage_insights)

        # Correlation analysis
        correlation_insights = self._analyze_test_correlations(results)
        insights.extend(correlation_insights)

        # Generate recommendations based on insights
        recommendations = self._generate_recommendations(insights, results)

        # Calculate quality score
        quality_score = self._calculate_quality_score(results, insights)

        # Generate trends
        trends = self._generate_trend_analysis(results, historical_depth)

        # Create summary
        summary = self._generate_summary(results, insights, recommendations)

        report = TestAnalysisReport(
            report_id=report_id,
            execution_context=context,
            analysis_timestamp=datetime.now(),
            summary=summary,
            insights=insights,
            recommendations=recommendations,
            trends=trends,
            quality_score=quality_score,
            metadata={
                "total_tests": len(results),
                "historical_depth_days": historical_depth,
                "analysis_version": "1.0"
            }
        )

        # Cache the report
        self.analysis_cache[report_id] = report

        return report

    def _analyze_failure_patterns(self, results: List[TestResult]) -> List[AnalysisInsight]:
        """Analyze patterns in test failures."""
        insights = []

        # Group failures by error patterns
        failure_patterns = defaultdict(list)
        for result in results:
            if result.status == TestStatus.FAILED and result.error_message:
                # Extract pattern from error message
                pattern = self._extract_error_pattern(result.error_message)
                failure_patterns[pattern].append(result)

        # Analyze significant patterns
        total_failures = len([r for r in results if r.status == TestStatus.FAILED])

        for pattern, failed_tests in failure_patterns.items():
            if len(failed_tests) >= 2:  # Pattern appears in multiple tests
                frequency = len(failed_tests) / total_failures if total_failures > 0 else 0

                severity = SeverityLevel.HIGH if frequency > 0.5 else \
                          SeverityLevel.MEDIUM if frequency > 0.2 else SeverityLevel.LOW

                insight = AnalysisInsight(
                    insight_id=f"pattern_{abs(hash(pattern))}",
                    analysis_type=AnalysisType.FAILURE_PATTERNS,
                    severity=severity,
                    title=f"Common Failure Pattern Detected",
                    description=f"Pattern '{pattern}' appears in {len(failed_tests)} tests ({frequency:.1%})",
                    affected_tests=[t.test_name for t in failed_tests],
                    metrics={
                        "pattern": pattern,
                        "frequency": frequency,
                        "occurrence_count": len(failed_tests)
                    },
                    confidence_score=min(frequency * 2, 1.0)
                )
                insights.append(insight)

        return insights

    def _analyze_performance_trends(self, results: List[TestResult], historical_depth: int) -> List[AnalysisInsight]:
        """Analyze performance trends in test execution."""
        insights = []

        # Group by test name and analyze trends
        test_performance = defaultdict(list)

        cutoff_date = datetime.now() - timedelta(days=historical_depth)

        for test_name in set(r.test_name for r in results):
            # Get historical data for this test
            historical_results = self.historical_data.get(test_name, [])
            recent_results = [r for r in historical_results
                            if r.start_time and r.start_time >= cutoff_date and r.execution_time]

            if len(recent_results) >= 5:  # Need enough data points
                execution_times = [r.execution_time for r in recent_results]

                # Calculate trend
                trend_slope = self._calculate_trend_slope(execution_times)
                avg_time = statistics.mean(execution_times)
                std_dev = statistics.stdev(execution_times) if len(execution_times) > 1 else 0

                # Detect performance degradation
                if trend_slope > 0.1 and avg_time > 1.0:  # Increasing trend and slow
                    severity = SeverityLevel.HIGH if trend_slope > 0.3 else SeverityLevel.MEDIUM

                    insight = AnalysisInsight(
                        insight_id=f"perf_trend_{abs(hash(test_name))}",
                        analysis_type=AnalysisType.PERFORMANCE_TRENDS,
                        severity=severity,
                        title=f"Performance Degradation Detected",
                        description=f"Test '{test_name}' shows increasing execution time trend",
                        affected_tests=[test_name],
                        metrics={
                            "trend_slope": trend_slope,
                            "avg_execution_time": avg_time,
                            "std_deviation": std_dev,
                            "sample_size": len(execution_times)
                        },
                        confidence_score=min(len(execution_times) / 20, 1.0)
                    )
                    insights.append(insight)

        return insights

    def _detect_flaky_tests(self, results: List[TestResult], historical_depth: int) -> List[AnalysisInsight]:
        """Detect tests that exhibit flaky behavior."""
        insights = []

        cutoff_date = datetime.now() - timedelta(days=historical_depth)

        # Analyze each test for flakiness
        for test_name in set(r.test_name for r in results):
            historical_results = self.historical_data.get(test_name, [])
            recent_results = [r for r in historical_results
                            if r.start_time and r.start_time >= cutoff_date]

            if len(recent_results) >= 10:  # Need sufficient sample size
                status_counts = Counter(r.status for r in recent_results)
                total_runs = len(recent_results)

                # Calculate flakiness score
                passed_runs = status_counts.get(TestStatus.PASSED, 0)
                failed_runs = status_counts.get(TestStatus.FAILED, 0)

                if passed_runs > 0 and failed_runs > 0:
                    flakiness_score = min(passed_runs, failed_runs) / total_runs
                    failure_rate = failed_runs / total_runs

                    # Detect flaky behavior
                    if flakiness_score > 0.1 and 0.1 < failure_rate < 0.9:
                        severity = SeverityLevel.HIGH if flakiness_score > 0.3 else \
                                  SeverityLevel.MEDIUM if flakiness_score > 0.2 else SeverityLevel.LOW

                        insight = AnalysisInsight(
                            insight_id=f"flaky_{abs(hash(test_name))}",
                            analysis_type=AnalysisType.FLAKY_TESTS,
                            severity=severity,
                            title=f"Flaky Test Detected",
                            description=f"Test '{test_name}' shows inconsistent results",
                            affected_tests=[test_name],
                            metrics={
                                "flakiness_score": flakiness_score,
                                "failure_rate": failure_rate,
                                "total_runs": total_runs,
                                "passed_runs": passed_runs,
                                "failed_runs": failed_runs
                            },
                            confidence_score=min(total_runs / 50, 1.0)
                        )
                        insights.append(insight)

        return insights

    def _analyze_test_coverage(self, results: List[TestResult]) -> List[AnalysisInsight]:
        """Analyze test coverage and identify gaps."""
        insights = []

        # Analyze assertion coverage
        assertion_counts = [len(r.assertion_results) for r in results if r.assertion_results]

        if assertion_counts:
            avg_assertions = statistics.mean(assertion_counts)
            min_assertions = min(assertion_counts)

            # Identify tests with low assertion coverage
            low_coverage_tests = [r.test_name for r in results
                                if r.assertion_results and len(r.assertion_results) < avg_assertions * 0.5]

            if low_coverage_tests:
                insight = AnalysisInsight(
                    insight_id="low_assertion_coverage",
                    analysis_type=AnalysisType.COVERAGE_ANALYSIS,
                    severity=SeverityLevel.MEDIUM,
                    title="Low Assertion Coverage Detected",
                    description=f"{len(low_coverage_tests)} tests have below-average assertion coverage",
                    affected_tests=low_coverage_tests,
                    metrics={
                        "avg_assertions": avg_assertions,
                        "min_assertions": min_assertions,
                        "affected_count": len(low_coverage_tests)
                    },
                    confidence_score=0.8
                )
                insights.append(insight)

        return insights

    def _analyze_test_correlations(self, results: List[TestResult]) -> List[AnalysisInsight]:
        """Analyze correlations between test failures."""
        insights = []

        failed_tests = [r for r in results if r.status == TestStatus.FAILED]

        if len(failed_tests) >= 2:
            # Look for tests that frequently fail together
            failure_pairs = []

            for i, test1 in enumerate(failed_tests):
                for test2 in failed_tests[i+1:]:
                    if test1.test_name != test2.test_name:
                        # Check if these tests often fail together historically
                        correlation_score = self._calculate_failure_correlation(test1.test_name, test2.test_name)

                        if correlation_score > 0.7:
                            failure_pairs.append((test1.test_name, test2.test_name, correlation_score))

            if failure_pairs:
                # Sort by correlation strength
                failure_pairs.sort(key=lambda x: x[2], reverse=True)

                for test1, test2, correlation in failure_pairs[:5]:  # Top 5 correlations
                    insight = AnalysisInsight(
                        insight_id=f"correlation_{abs(hash(test1 + test2))}",
                        analysis_type=AnalysisType.CORRELATION_ANALYSIS,
                        severity=SeverityLevel.MEDIUM,
                        title="Correlated Test Failures",
                        description=f"Tests '{test1}' and '{test2}' frequently fail together",
                        affected_tests=[test1, test2],
                        metrics={
                            "correlation_score": correlation,
                            "test_pair": [test1, test2]
                        },
                        confidence_score=correlation
                    )
                    insights.append(insight)

        return insights

    def _generate_recommendations(self,
                                insights: List[AnalysisInsight],
                                results: List[TestResult]) -> List[TestRecommendation]:
        """Generate actionable recommendations based on analysis insights."""
        recommendations = []

        for insight in insights:
            if insight.analysis_type == AnalysisType.FLAKY_TESTS:
                rec = TestRecommendation(
                    recommendation_id=f"fix_flaky_{insight.insight_id}",
                    recommendation_type=RecommendationType.FIX_FLAKY_TEST,
                    title=f"Fix Flaky Test: {insight.affected_tests[0] if insight.affected_tests else 'Unknown'}",
                    description="Investigate and fix inconsistent test behavior",
                    target_tests=insight.affected_tests,
                    priority=insight.severity,
                    effort_estimate="medium",
                    expected_impact="high",
                    implementation_steps=[
                        "Review test dependencies and timing issues",
                        "Add explicit waits or synchronization",
                        "Improve test data isolation",
                        "Consider splitting complex test scenarios"
                    ],
                    confidence_score=insight.confidence_score
                )
                recommendations.append(rec)

            elif insight.analysis_type == AnalysisType.PERFORMANCE_TRENDS:
                rec = TestRecommendation(
                    recommendation_id=f"optimize_perf_{insight.insight_id}",
                    recommendation_type=RecommendationType.OPTIMIZE_PERFORMANCE,
                    title=f"Optimize Performance: {insight.affected_tests[0] if insight.affected_tests else 'Unknown'}",
                    description="Address performance degradation in test execution",
                    target_tests=insight.affected_tests,
                    priority=insight.severity,
                    effort_estimate="medium",
                    expected_impact="medium",
                    implementation_steps=[
                        "Profile test execution to identify bottlenecks",
                        "Optimize database queries and operations",
                        "Review fixture data size and complexity",
                        "Consider parallel execution where appropriate"
                    ],
                    confidence_score=insight.confidence_score
                )
                recommendations.append(rec)

            elif insight.analysis_type == AnalysisType.COVERAGE_ANALYSIS:
                rec = TestRecommendation(
                    recommendation_id=f"improve_coverage_{insight.insight_id}",
                    recommendation_type=RecommendationType.IMPROVE_COVERAGE,
                    title="Improve Test Coverage",
                    description="Add more comprehensive assertions to tests",
                    target_tests=insight.affected_tests,
                    priority=SeverityLevel.LOW,
                    effort_estimate="low",
                    expected_impact="medium",
                    implementation_steps=[
                        "Review test scenarios for missing assertions",
                        "Add edge case testing",
                        "Verify error conditions are tested",
                        "Ensure all code paths are covered"
                    ],
                    confidence_score=insight.confidence_score
                )
                recommendations.append(rec)

            elif insight.analysis_type == AnalysisType.CORRELATION_ANALYSIS:
                rec = TestRecommendation(
                    recommendation_id=f"review_deps_{insight.insight_id}",
                    recommendation_type=RecommendationType.REVIEW_DEPENDENCIES,
                    title="Review Test Dependencies",
                    description="Investigate shared dependencies causing correlated failures",
                    target_tests=insight.affected_tests,
                    priority=SeverityLevel.MEDIUM,
                    effort_estimate="high",
                    expected_impact="high",
                    implementation_steps=[
                        "Analyze shared resources and dependencies",
                        "Improve test isolation",
                        "Consider splitting or refactoring tests",
                        "Add better error handling and cleanup"
                    ],
                    confidence_score=insight.confidence_score
                )
                recommendations.append(rec)

        return recommendations

    def _calculate_quality_score(self, results: List[TestResult], insights: List[AnalysisInsight]) -> float:
        """Calculate overall quality score for the test suite."""
        if not results:
            return 0.0

        # Base score from pass rate
        passed_tests = len([r for r in results if r.status == TestStatus.PASSED])
        pass_rate = passed_tests / len(results)

        # Deduct points for critical insights
        critical_penalty = len([i for i in insights if i.severity == SeverityLevel.CRITICAL]) * 0.2
        high_penalty = len([i for i in insights if i.severity == SeverityLevel.HIGH]) * 0.1
        medium_penalty = len([i for i in insights if i.severity == SeverityLevel.MEDIUM]) * 0.05

        # Calculate final score (0-100)
        quality_score = max(0, (pass_rate - critical_penalty - high_penalty - medium_penalty) * 100)

        return round(quality_score, 2)

    def _generate_trend_analysis(self, results: List[TestResult], historical_depth: int) -> Dict[str, Any]:
        """Generate trend analysis for test metrics."""
        trends = {}

        # Pass rate trend
        cutoff_date = datetime.now() - timedelta(days=historical_depth)
        all_historical = []

        for test_results in self.historical_data.values():
            all_historical.extend([r for r in test_results if r.start_time and r.start_time >= cutoff_date])

        if all_historical:
            # Group by day
            daily_results = defaultdict(list)
            for result in all_historical:
                day = result.start_time.date()
                daily_results[day].append(result)

            daily_pass_rates = []
            for day, day_results in sorted(daily_results.items()):
                passed = len([r for r in day_results if r.status == TestStatus.PASSED])
                pass_rate = passed / len(day_results)
                daily_pass_rates.append(pass_rate)

            if len(daily_pass_rates) >= 3:
                trends["pass_rate_trend"] = {
                    "slope": self._calculate_trend_slope(daily_pass_rates),
                    "current_rate": daily_pass_rates[-1] if daily_pass_rates else 0,
                    "data_points": len(daily_pass_rates)
                }

        return trends

    def _generate_summary(self,
                        results: List[TestResult],
                        insights: List[AnalysisInsight],
                        recommendations: List[TestRecommendation]) -> Dict[str, Any]:
        """Generate executive summary of analysis."""
        total_tests = len(results)
        passed_tests = len([r for r in results if r.status == TestStatus.PASSED])
        failed_tests = len([r for r in results if r.status == TestStatus.FAILED])

        critical_insights = len([i for i in insights if i.severity == SeverityLevel.CRITICAL])
        high_insights = len([i for i in insights if i.severity == SeverityLevel.HIGH])

        high_priority_recs = len([r for r in recommendations if r.priority == SeverityLevel.HIGH])

        return {
            "test_results": {
                "total_tests": total_tests,
                "passed_tests": passed_tests,
                "failed_tests": failed_tests,
                "pass_rate": passed_tests / total_tests if total_tests > 0 else 0
            },
            "insights_summary": {
                "total_insights": len(insights),
                "critical_issues": critical_insights,
                "high_priority_issues": high_insights,
                "insights_by_type": Counter(i.analysis_type for i in insights)
            },
            "recommendations_summary": {
                "total_recommendations": len(recommendations),
                "high_priority_recommendations": high_priority_recs,
                "recommendations_by_type": Counter(r.recommendation_type for r in recommendations)
            }
        }

    def _extract_error_pattern(self, error_message: str) -> str:
        """Extract meaningful pattern from error message."""
        if not error_message:
            return "unknown_error"

        # Common SQL error patterns
        sql_patterns = [
            (r"ORA-\d+", "oracle_error"),
            (r"ERROR \d+", "sql_server_error"),
            (r"Connection.*refused", "connection_error"),
            (r"Timeout", "timeout_error"),
            (r"Table.*doesn't exist", "missing_table"),
            (r"Column.*unknown", "missing_column"),
            (r"Syntax error", "syntax_error"),
            (r"Access denied", "permission_error")
        ]

        for pattern, category in sql_patterns:
            if re.search(pattern, error_message, re.IGNORECASE):
                return category

        # Generic pattern extraction
        words = error_message.lower().split()
        if len(words) >= 2:
            return f"{words[0]}_{words[1]}"

        return "generic_error"

    def _calculate_trend_slope(self, values: List[float]) -> float:
        """Calculate trend slope using simple linear regression."""
        if len(values) < 2:
            return 0.0

        n = len(values)
        x_values = list(range(n))

        # Calculate slope using least squares
        x_mean = statistics.mean(x_values)
        y_mean = statistics.mean(values)

        numerator = sum((x - x_mean) * (y - y_mean) for x, y in zip(x_values, values))
        denominator = sum((x - x_mean) ** 2 for x in x_values)

        if denominator == 0:
            return 0.0

        return numerator / denominator

    def _calculate_failure_correlation(self, test1: str, test2: str) -> float:
        """Calculate correlation between two tests' failure patterns."""
        results1 = self.historical_data.get(test1, [])
        results2 = self.historical_data.get(test2, [])

        if len(results1) < 5 or len(results2) < 5:
            return 0.0

        # Create timeline of failures
        failures1 = {r.start_time.date(): r.status == TestStatus.FAILED
                    for r in results1 if r.start_time}
        failures2 = {r.start_time.date(): r.status == TestStatus.FAILED
                    for r in results2 if r.start_time}

        # Find common dates
        common_dates = set(failures1.keys()) & set(failures2.keys())

        if len(common_dates) < 3:
            return 0.0

        # Calculate correlation
        both_failed = sum(1 for date in common_dates
                         if failures1[date] and failures2[date])
        either_failed = sum(1 for date in common_dates
                          if failures1[date] or failures2[date])

        if either_failed == 0:
            return 0.0

        return both_failed / either_failed

    def _initialize_pattern_library(self) -> Dict[str, Dict[str, Any]]:
        """Initialize library of known failure patterns and solutions."""
        return {
            "connection_error": {
                "description": "Database connection failures",
                "common_causes": ["Network issues", "Database downtime", "Connection pool exhaustion"],
                "solutions": ["Add connection retry logic", "Increase timeout values", "Check network connectivity"]
            },
            "timeout_error": {
                "description": "Query execution timeouts",
                "common_causes": ["Slow queries", "Database load", "Insufficient resources"],
                "solutions": ["Optimize queries", "Add indexes", "Increase timeout limits"]
            },
            "missing_table": {
                "description": "Referenced table doesn't exist",
                "common_causes": ["Incorrect test setup", "Missing fixtures", "Schema changes"],
                "solutions": ["Verify fixture setup", "Check test dependencies", "Update schema references"]
            },
            "syntax_error": {
                "description": "SQL syntax errors",
                "common_causes": ["Typos in SQL", "Database-specific syntax", "Dynamic query issues"],
                "solutions": ["Review SQL syntax", "Test with database client", "Add SQL validation"]
            }
        }

    def get_pattern_library(self) -> Dict[str, Dict[str, Any]]:
        """Get the current pattern library."""
        return self.pattern_library

    def add_pattern(self, pattern_name: str, pattern_info: Dict[str, Any]) -> None:
        """Add a new pattern to the library."""
        self.pattern_library[pattern_name] = pattern_info
        logger.info(f"Added new pattern to library: {pattern_name}")

    def export_analysis_report(self, report: TestAnalysisReport, format: str = "json") -> str:
        """Export analysis report in specified format."""
        if format.lower() == "json":
            return json.dumps({
                "report_id": report.report_id,
                "execution_context": report.execution_context,
                "analysis_timestamp": report.analysis_timestamp.isoformat(),
                "summary": report.summary,
                "quality_score": report.quality_score,
                "insights": [self._serialize_insight(i) for i in report.insights],
                "recommendations": [self._serialize_recommendation(r) for r in report.recommendations],
                "trends": report.trends,
                "metadata": report.metadata
            }, indent=2)
        else:
            raise ValueError(f"Unsupported export format: {format}")

    def _serialize_insight(self, insight: AnalysisInsight) -> Dict[str, Any]:
        """Serialize an insight to dictionary."""
        return {
            "insight_id": insight.insight_id,
            "analysis_type": insight.analysis_type.value,
            "severity": insight.severity.value,
            "title": insight.title,
            "description": insight.description,
            "affected_tests": insight.affected_tests,
            "metrics": insight.metrics,
            "confidence_score": insight.confidence_score,
            "timestamp": insight.timestamp.isoformat()
        }

    def _serialize_recommendation(self, recommendation: TestRecommendation) -> Dict[str, Any]:
        """Serialize a recommendation to dictionary."""
        return {
            "recommendation_id": recommendation.recommendation_id,
            "recommendation_type": recommendation.recommendation_type.value,
            "title": recommendation.title,
            "description": recommendation.description,
            "target_tests": recommendation.target_tests,
            "priority": recommendation.priority.value,
            "effort_estimate": recommendation.effort_estimate,
            "expected_impact": recommendation.expected_impact,
            "implementation_steps": recommendation.implementation_steps,
            "confidence_score": recommendation.confidence_score
        }


# Convenience functions
def analyze_workflow_execution(execution: WorkflowExecution, analyzer: IntelligentTestAnalyzer) -> TestAnalysisReport:
    """Analyze results from a workflow execution."""
    # Extract test results from workflow execution
    test_results = []

    for step_id, step_result in execution.step_results.items():
        if isinstance(step_result.get("result"), list):
            # Step returned list of test results
            test_results.extend(step_result["result"])
        elif hasattr(step_result.get("result"), "test_name"):
            # Step returned single test result
            test_results.append(step_result["result"])

    return analyzer.analyze_test_results(test_results, execution.execution_id)


def generate_improvement_plan(recommendations: List[TestRecommendation]) -> Dict[str, Any]:
    """Generate an improvement plan from recommendations."""
    # Sort by priority and impact
    high_priority = [r for r in recommendations if r.priority == SeverityLevel.HIGH]
    medium_priority = [r for r in recommendations if r.priority == SeverityLevel.MEDIUM]
    low_priority = [r for r in recommendations if r.priority == SeverityLevel.LOW]

    # Estimate total effort
    effort_map = {"low": 1, "medium": 3, "high": 8}
    total_effort = sum(effort_map.get(r.effort_estimate, 3) for r in recommendations)

    return {
        "total_recommendations": len(recommendations),
        "phases": {
            "phase_1_critical": {
                "recommendations": [r.recommendation_id for r in high_priority],
                "estimated_effort_points": sum(effort_map.get(r.effort_estimate, 3) for r in high_priority),
                "expected_impact": "high"
            },
            "phase_2_important": {
                "recommendations": [r.recommendation_id for r in medium_priority],
                "estimated_effort_points": sum(effort_map.get(r.effort_estimate, 3) for r in medium_priority),
                "expected_impact": "medium"
            },
            "phase_3_nice_to_have": {
                "recommendations": [r.recommendation_id for r in low_priority],
                "estimated_effort_points": sum(effort_map.get(r.effort_estimate, 3) for r in low_priority),
                "expected_impact": "low"
            }
        },
        "total_effort_points": total_effort,
        "implementation_timeline": "4-8 weeks" if total_effort > 20 else "2-4 weeks"
    }